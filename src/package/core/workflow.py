# -*- codeing =utf-8 -*-
# @Time :2025/12/13 20:30:00
# @Author : Scientist
# @File :workflow.py
# @Software :PyCharm

from typing import TypedDict, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
import json
from datetime import datetime
from package.generators import format_prompt, build_prompt_with_suggestions
from package.generators.SJTvirtual_subject import run_virtual_subject_simulation
from package.evaluators import format_all_items_prompt
from package.evaluators.SJTcontent_validity import convert_evaluation_results_to_csv, calculate_cvi_from_evaluation_results_single_expert
from package.evaluators.SJT_virtualSub_prompt import generate_filled_prompts_with_scores_only
from package.utils import (
    LLM_call, 
    LLM_call_concurrent, 
    get_project_root,
    sjt_responses_to_matrix, 
    TRAIT_ORDER
)
from package.utils import data_Ana as citc_analysis
from pathlib import Path
import pandas as pd
import numpy as np


class WorkflowState(TypedDict, total=False):
    # --- 1. åŸºç¡€é…ç½® ---
    trait_names: List[str]         # æ‰€æœ‰è¦å¤„ç†çš„ç‰¹è´¨åˆ—è¡¨
    model: ChatOpenAI              # æ¨¡å‹
    experts: List[ChatOpenAI]      # ä¸“å®¶åˆ—è¡¨
    # --- 2. ç”Ÿé¢˜å¾ªç¯---
    final_storage: List[Dict]      # ç”¨æ¥å­˜æ‰€æœ‰é€šè¿‡çš„é¢˜ç›® (æœ€ç»ˆç»“æœ)
    batch_count: int               # å½“å‰æ˜¯ç¬¬å‡ ä¸ªç‰¹è´¨
    target_batches: int            # å¤§äº”
    # --- 3. å†…å¾ªç¯å˜é‡ ---
    generated_items: List[Dict]    # å½“å‰æ­£åœ¨å¤„ç†çš„é¢˜ç›®
    evaluation_results: List[Dict] # å½“å‰çš„ä¸“å®¶è¯„åˆ†ç»“æœ
    evaluation_errors: List[Dict]  # è¯„ä¼°è¿‡ç¨‹ä¸­çš„é”™è¯¯ä¿¡æ¯
    passed_items: List[Dict]       # å½“å‰æ‰¹æ¬¡ä¸­ï¼ŒCVI è¾¾æ ‡çš„é¢˜ç›®
    low_cvi_items: List[Dict]      # å½“å‰æ‰¹æ¬¡ä¸­ï¼ŒCVI ä¸è¾¾æ ‡çš„é¢˜ç›®
    iteration: int                 # å½“å‰æ‰¹æ¬¡ä¿®äº†ç¬¬å‡ æ¬¡
    max_iterations: int            # å•æ‰¹æ¬¡æœ€å¤§ä¿®è®¢æ¬¡æ•°
    # --- 4. è™šæ‹Ÿè¢«è¯• ---
    virtual_subject_prompts_neo: List[Dict[str, str]]  # NEOè™šæ‹Ÿè¢«è¯•æç¤ºè¯
    virtual_subject_prompts_sjt: List[Dict[str, str]]  # SJTè™šæ‹Ÿè¢«è¯•æç¤ºè¯
    virtual_subject_responses_neo: List[Dict[str, Any]]  # NEOè™šæ‹Ÿè¢«è¯•å›ç­”
    virtual_subject_responses_sjt: List[Dict[str, Any]]  # SJTè™šæ‹Ÿè¢«è¯•å›ç­”
    # --- 5. åˆ†æ ---
    irt_bad_items: List[Dict[str, Any]]        # é—®é¢˜é¢˜ç›®åˆ—è¡¨ï¼ˆè‹¥ä½¿ç”¨ï¼‰
    irt_revision_prompt: str                   # å½“å‰ä¿®è®¢æç¤ºè¯
    irt_repair_mode: bool           # æ ‡è®°ï¼šå½“å‰æ˜¯å¦å¤„äºä¿®å¤æ¨¡å¼ (True/False)
    irt_iteration: int              # è®¡æ•°ï¼šå½“å‰ä¿®å¤æ˜¯ç¬¬å‡ è½® (é˜²æ­¢æ­»å¾ªç¯)
    irt_max_iterations: int         # é…ç½®ï¼šæœ€å¤§ä¿®å¤è½®æ¬¡ (å»ºè®®è®¾ä¸º 3)
    irt_prompt_queue: List[str]     # å¤šæ‰¹ä¿®è®¢æç¤ºé˜Ÿåˆ—
    irt_bad_items_queue: List[List[Dict[str, Any]]]  # å¤šæ‰¹åé¢˜é˜Ÿåˆ—ï¼ˆå¸¦traitï¼Œç”¨äºå›å¡«ï¼‰
    irt_prompt_queue: List[str] # å­˜å‚¨å¤šæ‰¹ä¿®è®¢æç¤ºï¼Œé€æ‰¹æ¶ˆè€—
    irt_repair_trait_name: str     # trait for current repair batch


def _normalize_item_id(raw_id: Any) -> str:
    """Normalize item id, stripping prefixes like QN_1 -> 1."""
    if raw_id is None:
        return ""
    raw_str = str(raw_id).strip()
    match = re.match(r"^Q[A-Za-z]+_(\d+)$", raw_str)
    return match.group(1) if match else raw_str


def _normalize_item(item: Any) -> Any:
    """Ensure item_id is consistent regardless of Item_ID/ItemId/QN_1 formats."""
    if not isinstance(item, dict):
        return item
    normalized = item.copy()
    raw_id = normalized.get("item_id") or normalized.get("Item_ID") or normalized.get("ItemId")
    if raw_id is not None:
        normalized["item_id"] = _normalize_item_id(raw_id)
    return normalized


def _sort_final_storage(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Sort items by trait order then numeric item_id to keep stable ordering."""
    trait_order = {name: idx for idx, (_, name) in enumerate(TRAIT_ORDER)}
    def _key(item: Dict[str, Any]):
        trait = item.get("trait", "")
        order = trait_order.get(trait, 999)
        try:
            iid = int(item.get("item_id"))
        except Exception:
            iid = 0
        return (order, iid)
    return sorted(items, key=_key)


def _normalize_evaluation_results(evaluation_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    normalized_results = []
    for er in evaluation_results or []:
        norm_res = [_normalize_item(item) for item in er.get("results", [])]
        normalized_results.append({**er, "results": norm_res})
    return normalized_results


def _parse_revision_prompt_traits(prompt: str) -> Dict[str, str]:
    """Parse Item_ID -> Trait from CITC revision prompt text."""
    if not prompt:
        return {}
    id_to_trait: Dict[str, str] = {}
    current_id = ""
    current_trait = ""
    for line in prompt.splitlines():
        line = line.strip()
        if line.startswith("Item_ID:"):
            current_id = _normalize_item_id(line.split(":", 1)[1].strip())
        elif line.startswith("Trait:"):
            current_trait = line.split(":", 1)[1].strip()
            if current_id and current_trait:
                id_to_trait[current_id] = current_trait
                current_id = ""
                current_trait = ""
    return id_to_trait


def generate_items_node(state: WorkflowState) -> WorkflowState:
    model = state.get("model")
    low_cvi_items = state.get("low_cvi_items", [])
    irt_repair_mode = state.get("irt_repair_mode", False)
    trait_names = state.get("trait_names", [])
    batch_count = state.get("batch_count", 0)
    if low_cvi_items:
        num_low_cvi = len(low_cvi_items)
        print(f"ğŸ”„  æ­£åœ¨æ ¹æ®ä¸“å®¶æ„è§ä¿®è®¢ {num_low_cvi} é“é¢˜ç›®...")
        modification_suggestions = []
        for item_info in low_cvi_items:
            item_info = _normalize_item(item_info)
            expert_evals = item_info.get("expert_evaluations", [])
            original_item = item_info.get("original_item", {})
            if expert_evals:
                expert_eval = expert_evals[0].get("evaluation", {})
                modification_suggestions.append({
                    "é¢˜ç›®ID": item_info.get("item_id", ""),
                    "åŸé¢˜": original_item,
                    "å†…å®¹æ•ˆåº¦è¯„ä¼°": expert_eval.get("å†…å®¹æ•ˆåº¦è¯„ä¼°", {})
                })
        prompt = build_prompt_with_suggestions(modification_suggestions)
    elif irt_repair_mode:
        irt_iteration = state.get("irt_iteration", 0)
        irt_max_iterations = state.get("irt_max_iterations", 1)
        print(f"ğŸ”§  è¿›å…¥ä¿®å¤æ¨¡å¼ï¼šç¬¬ {irt_iteration}/{irt_max_iterations} è½®...")
        prompt = state.get("irt_revision_prompt", "")
        if prompt:
            prompt = (
                f"{prompt}\n\n"
                "è¯·ä¸¥æ ¼ä¿ç•™åŸé¢˜çš„Item_IDï¼Œä¸è¦æ–°å¢æˆ–ä¿®æ”¹Item_IDï¼Œç¡®ä¿åç»­å¯ä»¥ç²¾ç¡®æ›¿æ¢ã€‚"
            )
        if not prompt:
            print("âš ï¸ è­¦å‘Š: ä¿®å¤æ¨¡å¼å·²å¯ç”¨ï¼Œä½†æœªæ‰¾åˆ°ä¿®è®¢æç¤ºè¯ï¼")
            state["generated_items"] = []
            return state
    else:
        if 0 <= batch_count < len(trait_names):
            trait_name = trait_names[batch_count]
            print(f"âœ¨ [æ–°æ‰¹æ¬¡] æ­£åœ¨ä¸ºç‰¹è´¨ [{trait_name}] ç”Ÿæˆé¢˜ç›®...")
            prompt = format_prompt(trait_name)
        else:
            print("âš ï¸ æ‰¹æ¬¡è®¡æ•°è¶…å‡ºèŒƒå›´æˆ–å·²å®Œæˆã€‚")
            state["generated_items"] = []
            return state
    try:
        items = LLM_call(prompt, model)
        normalized_items = [_normalize_item(item) for item in items]
        if irt_repair_mode:
            id_to_trait = _parse_revision_prompt_traits(state.get("irt_revision_prompt", ""))
            for item in normalized_items:
                item_id = item.get("item_id")
                if item_id is not None and "trait" not in item:
                    trait = id_to_trait.get(_normalize_item_id(item_id), "")
                    if trait:
                        item["trait"] = trait
        state["generated_items"] = normalized_items
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
        state["generated_items"] = []
    return state

def evaluate_items_node(state: WorkflowState) -> WorkflowState:
    experts = state.get("experts") or []
    items = state.get("generated_items", [])
    trait_names = state.get("trait_names", [])
    batch_count = state.get("batch_count", 0)
    trait_name = trait_names[batch_count] if 0 <= batch_count < len(trait_names) else ""
    irt_repair_mode = state.get("irt_repair_mode", False)
    if irt_repair_mode:
        id_to_trait = _parse_revision_prompt_traits(state.get("irt_revision_prompt", ""))
        items_by_trait = {}
        for idx, item in enumerate(items):
            item_trait = item.get("trait")
            if not item_trait:
                item_id = item.get("item_id")
                if item_id is not None:
                    item_trait = id_to_trait.get(_normalize_item_id(item_id), "")
            if not item_trait:
                item_trait = state.get("irt_repair_trait_name", trait_name)
            items_by_trait.setdefault(item_trait, []).append(item)
        prompt = format_all_items_prompt({"ç‰¹è´¨": items_by_trait})
    else:
        prompt = format_all_items_prompt({"ç‰¹è´¨": {trait_name: items}})
    if experts:
        expert = experts[0]
        evaluation_result = LLM_call(prompt, expert)
        if not isinstance(evaluation_result, list):
            evaluation_result = []
        evaluation_results = [{
            "expert_index": 1,
            "results": evaluation_result
        }]
        state["evaluation_results"] = evaluation_results
        state["evaluation_errors"] = []
    else:
        state["evaluation_results"] = []
        state["evaluation_errors"] = [{"error": "No experts available"}]
    return state

def convert_to_CVI_node(state: WorkflowState) -> WorkflowState:
    evaluation_results = _normalize_evaluation_results(state.get("evaluation_results", []))
    state["evaluation_results"] = evaluation_results
    generated_items = [_normalize_item(item) for item in state.get("generated_items", [])]
    state["generated_items"] = generated_items
    trait_names = state.get("trait_names", [])
    batch_count = state.get("batch_count", 0)
    trait_name = trait_names[batch_count] if 0 <= batch_count < len(trait_names) else ""
    irt_repair_mode = state.get("irt_repair_mode", False)
    irt_bad_items = state.get("irt_bad_items", [])
    print(f"--- æ­£åœ¨è®¡ç®—ç‰¹è´¨ [{trait_name}] çš„CVI ---")
    try:
        cvi_data, low_cvi_items, passed_items = calculate_cvi_from_evaluation_results_single_expert(
            evaluation_results,
            generated_items=generated_items
        )
        low_cvi_items = [_normalize_item(item) for item in low_cvi_items]
        passed_items = [_normalize_item(item) for item in passed_items]
        state["low_cvi_items"] = low_cvi_items
        if low_cvi_items:
            iteration = state.get("iteration", 0)
            state["iteration"] = iteration + 1
        final_storage = state.get("final_storage", [])
        if irt_repair_mode and irt_bad_items:
            # ä¿®å¤æ¨¡å¼ï¼šæŒ‰ item_id ç²¾ç¡®æ›¿æ¢æ—§é¢˜ç›®
            irt_bad_items_by_id = {
                _normalize_item_id(item.get("item_id", "")): item for item in irt_bad_items
                if item.get("item_id") is not None
            }
            original_bad_count = len(irt_bad_items_by_id)
            for new_item in passed_items:
                item_id = new_item.get("item_id")
                normalized_id = _normalize_item_id(item_id) if item_id is not None else ""
                old_item = irt_bad_items_by_id.pop(normalized_id, None)
                if old_item:
                    item_with_trait = new_item.copy()
                    item_with_trait["trait"] = old_item.get("trait", trait_name)
                    item_with_trait["item_id"] = old_item.get("item_id", new_item.get("item_id", ""))
                    final_storage.append(item_with_trait)
            remaining_bad_items = list(irt_bad_items_by_id.values())
            if remaining_bad_items:
                final_storage.extend(remaining_bad_items)
                print(f"âš ï¸ è­¦å‘Š: {len(remaining_bad_items)} é“æ—§é¢˜æœªè¢«æ›¿æ¢ï¼Œå·²ä¿ç•™åŸé¢˜")
            state["irt_bad_items"] = remaining_bad_items
            replaced_count = original_bad_count - len(irt_bad_items_by_id)
            print(f"âœ… æˆåŠŸä¿®å¤ {replaced_count} é“é¢˜ç›®ï¼Œå·²æ›¿æ¢åŸé¢˜ç›®")
        else:
            for item in passed_items:
                item_with_trait = item.copy()
                item_with_trait["trait"] = trait_name
                final_storage.append(item_with_trait)
        state["final_storage"] = _sort_final_storage(final_storage)
        state["passed_items"] = passed_items
        print(f"å·²è¯†åˆ« {len(low_cvi_items)} é“ä½CVIé¢˜ç›®ï¼Œ{len(passed_items)} é“åˆæ ¼é¢˜ç›®ï¼ˆå·²ç´¯ç§¯åˆ°æ€»åº“å­˜ï¼‰")
        csv_path = convert_evaluation_results_to_csv(evaluation_results)
        print(f"CSVæ–‡ä»¶å·²ä¿å­˜è‡³: {csv_path}")
    except Exception as e:
        print(f"è®¡ç®—CVIæ—¶å‡ºé”™: {e}")
        raise
    return state


def check_quality(state: WorkflowState) -> str:
    low_cvi_items = state.get("low_cvi_items", [])
    iteration = state.get("iteration", 0)
    max_iterations = state.get("max_iterations", 3)
    if low_cvi_items:
        if iteration >= max_iterations:
            print(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§CVIä¿®è®¢æ¬¡æ•°ï¼Œå¼ºåˆ¶å½’æ¡£")
            return "archive"
        print(f"ğŸ”„ CVIä¸åˆæ ¼ï¼Œè¿›å…¥ç¬¬ {state['iteration']} æ¬¡å†…å®¹ä¿®è®¢")
        return "revise"
    print("âœ… CVIè¯„ä¼°é€šè¿‡ï¼Œå‡†å¤‡å½’æ¡£")
    return "archive"

def check_quantity(state: WorkflowState) -> str:
    current_batch = state.get("batch_count", 0)
    target_batches = state.get("target_batches", 5)
    if current_batch < target_batches:
        return "next_batch"
    return "finish"


def virtual_subject_node(state: WorkflowState) -> WorkflowState:
    project_root = get_project_root()
    final_storage = state.get("final_storage", [])
    sjt_output_dir = project_root / "src" / "package" / "utils" / "sjt_outputs"
    sjt_output_dir.mkdir(parents=True, exist_ok=True)
    sjt_json_path = sjt_output_dir / "SJT_all_traits.json"
    traits_data: Dict[str, Dict[str, Any]] = {}
    trait_names = state.get("trait_names", [])
    for trait_name in trait_names:
        traits_data[trait_name] = {
            "trait": trait_name,
            "items": []
        }
    for item in final_storage:
        trait_name = item.get("trait", "")
        if trait_name and trait_name in traits_data:
            item_clean = {k: v for k, v in item.items() if k != "trait"}
            traits_data[trait_name]["items"].append(item_clean)
        else:
            print(f"âš ï¸ è­¦å‘Š: é¢˜ç›® {item.get('item_id', 'unknown')} ç¼ºå°‘ç‰¹è´¨ä¿¡æ¯ï¼Œå·²è·³è¿‡")
    sjt_data = {"traits": traits_data}
    with open(sjt_json_path, 'w', encoding='utf-8') as f:
        json.dump(sjt_data, f, ensure_ascii=False, indent=2)
    print(f"SJTé¢˜ç›®å·²ä¿å­˜è‡³: {sjt_json_path}")
    print("\n--- å¼€å§‹ç”Ÿæˆè™šæ‹Ÿè¢«è¯• ---")
    virtual_subjects_df = run_virtual_subject_simulation(
        n_subjects=300,
        driving_facet="N",
        mean=50.0,
        std=10.0,
        seed=1
    )
    csv_path = project_root / "src" / "package" / "generators" / "virtual_subjects.csv"
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    virtual_subjects_df.to_csv(csv_path, index=True, encoding='utf-8-sig')
    print(f"è™šæ‹Ÿè¢«è¯•åˆ†æ•°å·²ä¿å­˜è‡³: {csv_path}")
    # ç”ŸæˆNEOæç¤ºè¯
    print("\n--- ç”ŸæˆNEOæç¤ºè¯ ---")
    neo_prompts_output_path = project_root / "src" / "package" / "evaluators" / "filled_prompts_neo.json"
    virtual_subject_prompts_neo = generate_filled_prompts_with_scores_only(test_type="NEO")
    print(f"å·²ç”Ÿæˆ {len(virtual_subject_prompts_neo)} ä¸ªè™šæ‹Ÿè¢«è¯•çš„NEOæç¤ºè¯ï¼Œä¿å­˜è‡³: {neo_prompts_output_path}")
    print("\n--- ç”ŸæˆSJTæç¤ºè¯ ---")
    sjt_prompts_output_path = project_root / "src" / "package" / "evaluators" / "filled_prompts_sjt.json"
    virtual_subject_prompts_sjt = generate_filled_prompts_with_scores_only(test_type="SJT")
    print(f"å·²ç”Ÿæˆ {len(virtual_subject_prompts_sjt)} ä¸ªè™šæ‹Ÿè¢«è¯•çš„SJTæç¤ºè¯ï¼Œä¿å­˜è‡³: {sjt_prompts_output_path}")
    state["virtual_subject_prompts_neo"] = virtual_subject_prompts_neo
    state["virtual_subject_prompts_sjt"] = virtual_subject_prompts_sjt
    return state


def virtual_subject_response_node(state: WorkflowState) -> WorkflowState:
    model = state.get("model")
    virtual_subject_prompts_neo = state.get("virtual_subject_prompts_neo", [])
    virtual_subject_prompts_sjt = state.get("virtual_subject_prompts_sjt", [])
    if not virtual_subject_prompts_neo and not virtual_subject_prompts_sjt:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°è™šæ‹Ÿè¢«è¯•æç¤ºè¯ï¼Œè·³è¿‡å›ç­”")
        return state
    project_root = get_project_root()
    max_workers = 50
    # å¤„ç†NEOå›ç­”
    neo_responses: List[Dict[str, Any]] = []
    if virtual_subject_prompts_neo:
        print(f"\n--- å¼€å§‹ç”ŸæˆNEOå›ç­”ï¼ˆå…± {len(virtual_subject_prompts_neo)} ä¸ªè¢«è¯•ï¼Œå¹¶å‘æ•°: {max_workers}ï¼‰---")
        neo_prompts = [(item["prompt"], model) for item in virtual_subject_prompts_neo]
        neo_results = LLM_call_concurrent(neo_prompts, max_workers=max_workers)
        for idx, (prompt_item, result) in enumerate(zip(virtual_subject_prompts_neo, neo_results)):
            subject_id = str(prompt_item["è¢«è¯•ID"])
            if result.get("success", False):
                response_text = result.get("result", [])
                if isinstance(response_text, list) and len(response_text) > 0:
                    response_text = response_text[0] if len(response_text) == 1 else response_text
                neo_responses.append({
                    "è¢«è¯•ID": subject_id,
                    "test_type": "NEO",
                    "response": response_text
                })
            else:
                error = result.get("error", "æœªçŸ¥é”™è¯¯")
                neo_responses.append({
                    "è¢«è¯•ID": subject_id,
                    "test_type": "NEO",
                    "response": f"ç”Ÿæˆå¤±è´¥: {error}"
                })
        # ä¿å­˜NEOå›ç­”
        neo_output_path = project_root / "src" / "package" / "evaluators" / "neo_responses.json"
        neo_output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(neo_output_path, 'w', encoding='utf-8') as f:
            json.dump(neo_responses, f, ensure_ascii=False, indent=2)
        print(f"å·²ç”Ÿæˆ {len(neo_responses)} æ¡NEOå›ç­”ï¼Œä¿å­˜è‡³: {neo_output_path}")
    # å¤„ç†SJTå›ç­”
    sjt_responses: List[Dict[str, Any]] = []
    if virtual_subject_prompts_sjt:
        print(f"\n--- å¼€å§‹ç”ŸæˆSJTå›ç­”ï¼ˆå…± {len(virtual_subject_prompts_sjt)} ä¸ªè¢«è¯•ï¼Œå¹¶å‘æ•°: {max_workers}ï¼‰---")
        sjt_prompts = [(item["prompt"], model) for item in virtual_subject_prompts_sjt]
        sjt_results = LLM_call_concurrent(sjt_prompts, max_workers=max_workers)
        for idx, (prompt_item, result) in enumerate(zip(virtual_subject_prompts_sjt, sjt_results)):
            subject_id = str(prompt_item["è¢«è¯•ID"])
            if result.get("success", False):
                response_text = result.get("result", [])
                if isinstance(response_text, list) and len(response_text) > 0:
                    response_text = response_text[0] if len(response_text) == 1 else response_text
                sjt_responses.append({
                    "è¢«è¯•ID": subject_id,
                    "test_type": "SJT",
                    "response": response_text
                })
            else:
                error = result.get("error", "æœªçŸ¥é”™è¯¯")
                sjt_responses.append({
                    "è¢«è¯•ID": subject_id,
                    "test_type": "SJT",
                    "response": f"ç”Ÿæˆå¤±è´¥: {error}"
                })
        sjt_output_path = project_root / "src" / "package" / "evaluators" / "sjt_responses.json"
        sjt_output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(sjt_output_path, 'w', encoding='utf-8') as f:
            json.dump(sjt_responses, f, ensure_ascii=False, indent=2)
        print(f"å·²ç”Ÿæˆ {len(sjt_responses)} æ¡SJTå›ç­”ï¼Œä¿å­˜è‡³: {sjt_output_path}")
    state["virtual_subject_responses_neo"] = neo_responses
    state["virtual_subject_responses_sjt"] = sjt_responses
    return state


def analysis_node(state: WorkflowState) -> WorkflowState:
    """æ‰§è¡ŒCITCåˆ†æï¼Œç”Ÿæˆä¿®è®¢æç¤ºï¼ˆä¸åˆ é™¤é¢˜ç›®ã€ä¸è·‘2PL IRTï¼‰ã€‚"""
    print("\n--- å¼€å§‹æ‰§è¡ŒCITCåˆ†æ ---")
    try:
        data = citc_analysis.sjt_responses_to_matrix()
        citc_df = citc_analysis.citc_by_trait(data, items_per_trait=None, corrected=True)
        project_root = get_project_root()
        output_dir = project_root / "output" / "CITC_analysis"
        output_dir.mkdir(parents=True, exist_ok=True)
        report_path = output_dir / "CITC_åˆ†ææŠ¥å‘Š.csv"
        citc_df.to_csv(report_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ CITCåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        prompt_paths, prompt_item_ids = citc_analysis.generate_citc_prompts_to_files(
            citc_df, batch_size=5, output_dir=output_dir
        )
        if prompt_paths:
            prompt_texts = [Path(p).read_text(encoding="utf-8") for p in prompt_paths]
            first_prompt = prompt_texts.pop(0)
            state["irt_prompt_queue"] = prompt_texts
            state["irt_revision_prompt"] = first_prompt
            # åˆ é™¤åé¢˜å¹¶è®°å½•æŒ‰æ‰¹æ¬¡çš„åé¢˜é˜Ÿåˆ—
            bad_df = citc_df[(citc_df["citc"].isna()) | (citc_df["citc"] < 0.5)]
            bad_qids_all = bad_df["item"].astype(str).tolist()
            name_to_code = {name: code for code, name in TRAIT_ORDER}
            final_storage = state.get("final_storage", [])
            kept, removed = [], []
            matched_bad_ids = set()
            for item in final_storage:
                trait = item.get("trait", "")
                item_id_num = _normalize_item_id(item.get("item_id", ""))
                trait_code = name_to_code.get(trait, "")
                item_id_str = f"Q{trait_code}_{item_id_num}" if trait_code and item_id_num else str(item_id_num)
                if item_id_str in bad_qids_all:
                    matched_bad_ids.add(item_id_str)
                    removed.append(item)
                else:
                    kept.append(item)
            state["final_storage"] = _sort_final_storage(kept)
            removed_map: Dict[str, Dict[str, Any]] = {}
            for item in removed:
                trait = item.get("trait", "")
                item_id_num = _normalize_item_id(item.get("item_id", ""))
                trait_code = name_to_code.get(trait, "")
                item_id_str = f"Q{trait_code}_{item_id_num}" if trait_code and item_id_num else str(item_id_num)
                removed_map[item_id_str] = item
            bad_items_queue: List[List[Dict[str, Any]]] = []
            for batch_ids in prompt_item_ids:
                batch_items = [removed_map[qid] for qid in batch_ids if qid in removed_map]
                bad_items_queue.append(batch_items)
            current_bad = bad_items_queue.pop(0) if bad_items_queue else []
            state["irt_bad_items_queue"] = bad_items_queue
            state["irt_bad_items"] = current_bad
            state["irt_repair_trait_name"] = current_bad[0].get("trait", "") if current_bad else ""
            state["irt_repair_mode"] = True
            print(f"ğŸ§¹ åé¢˜åŒ¹é…å‘½ä¸­ {len(matched_bad_ids)} / {len(bad_qids_all)}")
            print(f"ğŸ“„ å·²ç”Ÿæˆ {len(prompt_paths)} ä¸ª CITC ä¿®è®¢æç¤ºï¼Œå·²ä»åº“å­˜åˆ é™¤ {len(removed)} é“é—®é¢˜é¢˜ç›®")
        else:
            print("âœ… æ‰€æœ‰é¢˜ç›®çš„CITCå‡åœ¨ 0.5 ä»¥ä¸Š")
            state["irt_revision_prompt"] = ""
            state["irt_bad_items"] = []
            state["irt_repair_mode"] = False
            state["irt_iteration"] = 0

        params_path = output_dir / "CITCå‚æ•°.csv"
        citc_df[['trait', 'item', 'citc', 'quality']].to_csv(
            params_path, index=False, encoding='utf-8-sig'
        )
        print("âœ… CITCåˆ†æå®Œæˆ")
    except Exception as e:
        print(f"âŒ CITCåˆ†æå‡ºé”™: {e}")
        state["irt_analysis_error"] = str(e)
    return state

def _update_sjt_all_traits_file(state: WorkflowState) -> None:
    """æ›´æ–°SJT_all_traits.jsonæ–‡ä»¶ï¼Œä½¿å…¶ä¸final_storageä¿æŒä¸€è‡´"""
    project_root = get_project_root()
    final_storage = state.get("final_storage", [])
    sjt_output_dir = project_root / "src" / "package" / "utils" / "sjt_outputs"
    sjt_output_dir.mkdir(parents=True, exist_ok=True)
    sjt_json_path = sjt_output_dir / "SJT_all_traits.json"
    traits_data: Dict[str, Dict[str, Any]] = {}
    trait_names = state.get("trait_names", [])
    for trait_name in trait_names:
        traits_data[trait_name] = {
            "trait": trait_name,
            "items": []
        }
    for item in final_storage:
        trait_name = item.get("trait", "")
        if trait_name and trait_name in traits_data:
            item_clean = {k: v for k, v in item.items() if k != "trait"}
            traits_data[trait_name]["items"].append(item_clean)
    sjt_data = {"traits": traits_data}
    with open(sjt_json_path, 'w', encoding='utf-8') as f:
        json.dump(sjt_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ å·²æ›´æ–°SJTé¢˜ç›®æ–‡ä»¶: {sjt_json_path}ï¼ˆå…± {len(final_storage)} é“é¢˜ç›®ï¼‰")


def check_irt_repair(state: WorkflowState) -> str:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤"""
    irt_iteration = state.get("irt_iteration", 0)
    irt_max_iterations = state.get("irt_max_iterations", 3)
    irt_revision_prompt = state.get("irt_revision_prompt", "")
    prompt_queue = state.get("irt_prompt_queue", [])
    bad_queue = state.get("irt_bad_items_queue", [])

    if not irt_revision_prompt:
        if prompt_queue:
            next_prompt = prompt_queue.pop(0)
            state["irt_revision_prompt"] = next_prompt
            state["irt_prompt_queue"] = prompt_queue
            # åŒæ­¥ä¸‹ä¸€æ‰¹åé¢˜
            if bad_queue:
                next_bad = bad_queue.pop(0)
                state["irt_bad_items"] = next_bad
                state["irt_bad_items_queue"] = bad_queue
                state["irt_repair_trait_name"] = next_bad[0].get("trait", "") if next_bad else ""
        else:
            # æ²¡æœ‰æ›´å¤šæç¤ºè¯ï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªæ¢å¤çš„åé¢˜
            current_bad = state.get("irt_bad_items", [])
            final_storage = state.get("final_storage", [])
            if current_bad:
                # åªæ¢å¤é‚£äº›è¿˜æ²¡æœ‰è¢«æˆåŠŸä¿®å¤çš„é¢˜ç›®
                existing_item_ids = {
                    (item.get("item_id"), item.get("trait")) 
                    for item in final_storage
                }
                to_restore = [
                    item for item in current_bad
                    if (item.get("item_id"), item.get("trait")) not in existing_item_ids
                ]
                if to_restore:
                    final_storage.extend(to_restore)
                    state["final_storage"] = _sort_final_storage(final_storage)
                    print(f"âš ï¸ ä¿®å¤å¤±è´¥ï¼Œå·²æ¢å¤ {len(to_restore)} é“æœªä¿®å¤çš„åŸé¢˜ç›®åˆ°åº“å­˜")
            # CITCä¿®å¤å®Œæˆåï¼Œé‡æ–°ä¿å­˜SJT_all_traits.jsonä»¥ä¿æŒä¸€è‡´æ€§
            _update_sjt_all_traits_file(state)
            return "finish"

    if irt_iteration >= irt_max_iterations:
        print(f"å·²è¾¾åˆ°æœ€å¤§ä¿®å¤æ¬¡æ•° ({irt_max_iterations} æ¬¡)ï¼Œåœæ­¢ä¿®å¤")
        # åªæ¢å¤é‚£äº›è¿˜æ²¡æœ‰è¢«æˆåŠŸä¿®å¤çš„é¢˜ç›®
        current_bad = state.get("irt_bad_items", [])
        final_storage = state.get("final_storage", [])
        restored_count = 0
        
        # æ£€æŸ¥å¹¶æ¢å¤å½“å‰æ‰¹æ¬¡ä¸­æœªä¿®å¤çš„é¢˜ç›®
        if current_bad:
            # æ£€æŸ¥å“ªäº›é¢˜ç›®è¿˜æ²¡æœ‰è¢«æ›¿æ¢ï¼ˆé€šè¿‡ item_id å’Œ trait åŒ¹é…ï¼‰
            existing_item_ids = {
                (item.get("item_id"), item.get("trait")) 
                for item in final_storage
            }
            to_restore = [
                item for item in current_bad
                if (item.get("item_id"), item.get("trait")) not in existing_item_ids
            ]
            if to_restore:
                final_storage.extend(to_restore)
                restored_count += len(to_restore)
        if bad_queue:
            for bad_batch in bad_queue:
                final_storage.extend(bad_batch)
                restored_count += len(bad_batch)
            state["irt_bad_items_queue"] = []
        
        if restored_count > 0:
            state["final_storage"] = _sort_final_storage(final_storage)
            print(f"âš ï¸ ä¿®å¤å¤±è´¥ï¼Œå·²æ¢å¤ {restored_count} é“æœªä¿®å¤çš„åŸé¢˜ç›®åˆ°åº“å­˜ï¼Œç¡®ä¿é¢˜ç›®æ•°é‡")
        
        # CITCä¿®å¤å®Œæˆåï¼Œé‡æ–°ä¿å­˜SJT_all_traits.jsonä»¥ä¿æŒä¸€è‡´æ€§
        _update_sjt_all_traits_file(state)
        
        return "finish"
    state["irt_repair_mode"] = True
    state["irt_iteration"] = irt_iteration + 1
    state["irt_prompt_queue"] = prompt_queue
    state["irt_bad_items_queue"] = bad_queue
    print(f"ğŸ”„ å¼€å§‹ç¬¬ {state['irt_iteration']}/{irt_max_iterations} è½®ä¿®å¤")
    return "repair"

def accumulator_node(state: WorkflowState) -> WorkflowState:
    current_passed = state.get("passed_items", [])
    final_storage = state.get("final_storage", [])
    current_batch = state.get("batch_count", 0)
    irt_repair_mode = state.get("irt_repair_mode", False)
    if irt_repair_mode:
        new_batch_count = current_batch
        print(f"ğŸ”§ [å½’æ¡£-ä¿®å¤] IRTä¿®å¤é¢˜ç›®å·²å…¥åº“ï¼Œæ‰¹æ¬¡ä¿æŒ: {new_batch_count}")
    else:
        new_batch_count = current_batch + 1
        print(f"\n [å½’æ¡£å®Œæˆ] ç¬¬ {new_batch_count} ä¸ªæ‰¹æ¬¡ç»“æŸã€‚")
        print(f"æœ¬æ‰¹åˆæ ¼: {len(current_passed)} é¢˜|æ€»åº“å­˜: {len(final_storage)} é¢˜")

    return {
        "final_storage": final_storage,
        "batch_count": new_batch_count,
        "generated_items": [],
        "passed_items": [],
        "low_cvi_items": [],
        "evaluation_results": [],
        "iteration": 0,
        # æ¸…ç©ºå½“å‰æç¤ºï¼Œä¸‹ä¸€è½®ç”±é˜Ÿåˆ—è¡¥å……
        "irt_repair_mode": False,
        "irt_revision_prompt": "",
    }


def create_sjt_workflow(model: ChatOpenAI = None) -> StateGraph:
    workflow = StateGraph(WorkflowState)
    workflow.add_node("generate_items", generate_items_node)
    workflow.add_node("evaluate_items", evaluate_items_node)
    workflow.add_node("convert_to_CVI", convert_to_CVI_node) # è®¡ç®—CVIèŠ‚ç‚¹
    workflow.add_node("accumulator", accumulator_node)       # å½’æ¡£èŠ‚ç‚¹
    workflow.add_node("virtual_subject", virtual_subject_node) # è™šæ‹Ÿè¢«è¯•èŠ‚ç‚¹
    workflow.add_node("virtual_subject_response", virtual_subject_response_node) # è™šæ‹Ÿè¢«è¯•å›ç­”èŠ‚ç‚¹
    workflow.add_node("analysis", analysis_node) # åˆ†æèŠ‚ç‚¹
    workflow.set_entry_point("generate_items")
    workflow.add_edge("generate_items", "evaluate_items")
    workflow.add_edge("evaluate_items", "convert_to_CVI")
    workflow.add_conditional_edges(
        "convert_to_CVI",
        check_quality,
        {
            "revise": "generate_items",
            "archive": "accumulator",
        }
    )
    workflow.add_conditional_edges(
        "accumulator",
        check_quantity,
        {
            "next_batch": "generate_items",
            "finish": "virtual_subject"
        }
    )
    workflow.add_edge("virtual_subject", "virtual_subject_response")
    workflow.add_edge("virtual_subject_response", "analysis")
    workflow.add_conditional_edges(
        "analysis",
        check_irt_repair,
        {
            "repair": "generate_items",
            "finish": END
        }
    )
    return workflow.compile()


def create_sjt_repair_workflow(model: ChatOpenAI = None) -> StateGraph:
    """Repair-only workflow that skips virtual subjects and CITC analysis."""
    workflow = StateGraph(WorkflowState)
    workflow.add_node("generate_items", generate_items_node)
    workflow.add_node("evaluate_items", evaluate_items_node)
    workflow.add_node("convert_to_CVI", convert_to_CVI_node)
    workflow.add_node("accumulator", accumulator_node)
    workflow.set_entry_point("generate_items")
    workflow.add_edge("generate_items", "evaluate_items")
    workflow.add_edge("evaluate_items", "convert_to_CVI")
    workflow.add_conditional_edges(
        "convert_to_CVI",
        check_quality,
        {
            "revise": "generate_items",
            "archive": "accumulator",
        }
    )
    workflow.add_conditional_edges(
        "accumulator",
        check_irt_repair,
        {
            "repair": "generate_items",
            "finish": END,
        }
    )
    return workflow.compile()

def run_workflow(
    trait_names: List[str],
    model: ChatOpenAI = None,
    experts: List[ChatOpenAI] = None,
    irt_max_iterations: int = 1,
) -> Dict[str, Any]:
    workflow = create_sjt_workflow(model)
    initial_state: WorkflowState = {
        "target_trait": trait_names[0] if trait_names else "",
        "trait_names": trait_names,
        "model": model,
        "experts": experts,
        "target_batches": 5,
        "batch_count": 0,
        "final_storage": [],
        "generated_items": [],
        "low_cvi_items": [],
        "iteration": 0,
        # ä¿®å¤ç›¸å…³å­—æ®µåˆå§‹åŒ–
        "irt_repair_mode": False,
        "irt_iteration": 0,
        "irt_max_iterations": irt_max_iterations,
        "irt_bad_items": [],
        "irt_revision_prompt": "",
        "irt_prompt_queue": [],
        "irt_repair_trait_name": ""
    }
    result = workflow.invoke(
        initial_state,
        config={"recursion_limit": 150}
    )
    final_items = result.get("final_storage", [])
    return {"final_items": final_items}

def main():
    from dotenv import load_dotenv
    from package.utils import TRAIT_ORDER
    from package.evaluators.SJTcontent_validity import get_content_validity_experts
    load_dotenv()
    model = ChatOpenAI(model="gpt-5-mini", temperature=0.5, max_tokens=7000)
    trait_names = [name for _, name in TRAIT_ORDER]
    experts = get_content_validity_experts()
    result = run_workflow(
        trait_names=trait_names,
        model=model,
        experts=experts,
        irt_max_iterations=3,
    )
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    project_root = get_project_root()
    output_dir = project_root / "output" / "workflow_results"
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = output_dir / f"workflow_result_{timestamp}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    print(f"ğŸ“„ å·¥ä½œæµç»“æœå·²ä¿å­˜è‡³: {result_file}")
    print(f"\nå·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")
if __name__ == "__main__":
    main()
